# SoC_ChatGPT_from_Scratch: Project Development Journal

This repository documents my progress in the **SoC project â€“ ChatGPT from Scratch**.

---

## Progress Summary

### Week 1: Python & Data Science Basics
- Refreshed core Python concepts.
- Covered Numpy, Pandas, and Matplotlib for data handling and visualization.

---

### Week 2 & 3: Neural Networks
- Studied neural networks from scratch using the StatQuest YouTube Playlist.
- Covered key topics: perceptrons, activation functions, loss functions, and forward pass (videos 74 to 84).

---

### Week 4: Backpropagation
- Continued with the StatQuest playlist (videos 86 to 91).
- Learned about the concept and math behind backpropagation and gradient descent.

---

### Week 5: Introduction to PyTorch
- Began working with PyTorch using online tutorials.
- Practiced creating and manipulating tensors.
- Started experimenting with building simple neural networks using `torch.nn`.

---

### Week 6 & 7: Transformers & Project Implementation
- Studied the "Attention is All You Need" paper and Andrej Karpathy's "Let's build GPT" tutorial to understand the Transformer architecture.
- Implemented a decoder-only Transformer model from scratch in PyTorch.
- Prepared two datasets for training: the script from *Mission: Impossible - Fallout* and text from Sherlock Holmes stories.
- Developed two Jupyter Notebooks (`Skynet.ipynb` and `Skynet_SH.ipynb`) to train and run the models.

---

## Final Status: Project Complete

- Successfully trained the model on the *Mission: Impossible* script.
- Saved and loaded pre-trained model weights for the Sherlock Holmes model (`farturConanDoyle.pth`).
- Generated sample text from both models, demonstrating that they learned the distinct styles of the source material.
- The project is complete and the final report is available in the main **[README.md](../blob/main/README.md)** file.
